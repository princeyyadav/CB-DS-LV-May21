{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4490b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cc178451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \n",
    "    def __init__(self, ksize, filters, input_shape, activation, stride=1, padding=0):\n",
    "        self.ksize = ksize\n",
    "        self.filters = filters\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.input_shape = input_shape\n",
    "        self.channels = input_shape[-1]\n",
    "        self.activation = activation\n",
    "        self.kernels = []\n",
    "        for i in range(self.filters):\n",
    "            k = np.random.randn(ksize, ksize, self.channels)\n",
    "            self.kernels.append(k)\n",
    "        self.bias = np.random.randn(1,self.filters)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _rotate(inp):\n",
    "        assert len(inp.shape)==4, f\"No. of dim in inp not equal to 4, got {inp.shape}\"\n",
    "        return np.flip(inp, axis=(1,2))\n",
    "\n",
    "    @staticmethod\n",
    "    def _convolution_op_helper(inp, kernel, stride=1):\n",
    "        # inp shape -> 4 dim\n",
    "        assert len(inp.shape)==4, f\"No. of dim in inp not equal to 4, got {inp.shape}\"\n",
    "        # kernel shouldhave 4 dim\n",
    "        assert len(kernel.shape)==4, f\"No. of dim in kernel not equal to 4, got {kernel.shape}\"\n",
    "\n",
    "        # no. of chanels in kernel and that in inp it should be same\n",
    "        assert inp.shape[-1] == kernel.shape[-1], f\"Mismatch in no. of channels in inp and kernel, got inp {inp.shape[-1]}, kernel {kernel.shape[-1]}\"\n",
    "        # non-square kernels are not allowed\n",
    "        assert kernel.shape[1] == kernel.shape[2], f\"dim 0 of kernel doesn't match dim 1, got {kernel.shape}\"\n",
    "        # inp shape square\n",
    "        assert inp.shape[1]>=kernel.shape[1] and inp.shape[2]>=kernel.shape[2], f\"Inp map dim(1,2) < kernel dim(1,2), got inp map dim 1, 2 {inp.shape[1:-1]}, kernel dim 1,2 {kernel.shape[1:-1]}\"\n",
    "\n",
    "        # flip the kernel\n",
    "        kernel = Conv2D._rotate(kernel)\n",
    "\n",
    "        oup = []\n",
    "        start_rloc = 0\n",
    "        end_rloc = kernel.shape[1]\n",
    "        while end_rloc <= inp.shape[1]:\n",
    "            output = []\n",
    "            start_cloc = 0\n",
    "            end_cloc = kernel.shape[2]\n",
    "            while end_cloc <= inp.shape[2]:\n",
    "                conv = (inp[:,start_rloc:end_rloc, start_cloc:end_cloc]*kernel).sum(axis=(1,2,3))\n",
    "                output.append(conv)\n",
    "\n",
    "                start_cloc += stride\n",
    "                end_cloc += stride\n",
    "            oup.append(output)\n",
    "            start_rloc += stride\n",
    "            end_rloc += stride\n",
    "        return np.moveaxis(oup, -1, 0)\n",
    "    \n",
    "    def _convolution_op(self, inp):\n",
    "        output = []\n",
    "        for kernel in self.kernels:\n",
    "            o = Conv2D._convolution_op_helper(inp, np.expand_dims(kernel, axis=0), self.stride)\n",
    "            output.append(o)\n",
    "        return np.stack(output, axis=-1)\n",
    "            \n",
    "    @staticmethod\n",
    "    def _pad(inp, pad_width):   \n",
    "        assert len(inp.shape)==4, f\"No. of dim in inp not equal to 4, got {inp.shape}\"\n",
    "        return np.pad(inp, ((0,0), (pad_width,pad_width), (pad_width,pad_width), (0,0)))\n",
    "\n",
    "    @staticmethod\n",
    "    def _inside_pad(inp, pad_width):\n",
    "        assert len(inp.shape)==4, f\"No. of dim in inp not equal to 4, got {inp.shape}\"\n",
    "        ix = np.repeat(np.arange(1, inp.shape[1]), pad_width)\n",
    "        inp = np.insert(inp, ix, 0, axis=1)\n",
    "        return np.insert(inp, ix, 0, axis=2)\n",
    "        \n",
    "\n",
    "    def eval(self, X):\n",
    "        o_ = self._convolution_op(X) + self.bias\n",
    "        return self.activation(o_)\n",
    "\n",
    "    def grad_input(self, X):\n",
    "        g1 = self.activation.grad_input( self.dot(X) )\n",
    "        g2 = self.dot.grad_input(X)\n",
    "        return np.einsum('mij,mjk->mik', g1, g2)\n",
    "\n",
    "    def grad_parameters(self, X):\n",
    "        da_dI = self.activation.grad_input(self.dot(X))\n",
    "        dI_dw = self.dot.grad_w(X)\n",
    "        da_dw = np.einsum('mij,mjkl->mikl', da_dI, dI_dw)\n",
    "\n",
    "        dI_db = self.dot.grad_b(X)\n",
    "        # print(da_dI.shape, dI_dw.shape, dI_db.shape)\n",
    "        da_db = np.einsum('mij,mjk->mik',  da_dI, dI_db)\n",
    "        return da_dw, da_db\n",
    "\n",
    "    def backprop_grad(self, grad_loss, grad):\n",
    "        dL_dwi = np.einsum('mij,mjkl->mikl', grad_loss, grad['w']).sum(axis=0)\n",
    "        dL_dbi = np.einsum('mij,mjk->mik', grad_loss, grad['b']).sum(axis=0)\n",
    "        grad_loss = np.einsum('mij,mjk->mik', grad_loss, grad['input'])\n",
    "        return dL_dwi, dL_dbi, grad_loss\n",
    "        \n",
    "    def update(self, grad, optimizer):\n",
    "        \"\"\" grad: (dL_dwi, dL_dbi)\"\"\"\n",
    "        self.dot.W = optimizer.minimize(self.dot.W, grad[0])\n",
    "        self.dot.b = optimizer.minimize(self.dot.b, grad[1])\n",
    "        \n",
    "    def get_parameter_shape(self):\n",
    "        return self.dot.get_parameter_shape()\n",
    "    \n",
    "    def get_output_shape(self):\n",
    "        m, n, k, p, s = self.input_shape[1], input_shape[2], self.ksize, self.padding, self.stride\n",
    "        return (m-k+(2*p)//s)+1, (n-k+(2*p)//s)+1\n",
    "    \n",
    "    def get_total_parameters(self):\n",
    "        return np.prod((len(self.kernels), *self.kernels[0].shape)) + np.prod(self.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "56a4340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.eval(X)\n",
    "\n",
    "    def eval(self, X):\n",
    "        return 1/((np.e**-X) + 1)\n",
    "\n",
    "    def grad_input(self, X):\n",
    "        I = np.identity(X.shape[1])\n",
    "        b = self.eval(X)*(1-self.eval(X)) # same shape as X\n",
    "        return np.einsum('ij,mi->mij', I, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "05c0a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.ones((10, 10,10, 3))\n",
    "l1 = Conv2D(ksize=5, filters=2, input_shape=inp.shape, activation=Sigmoid(), stride=1, padding=0)\n",
    "o = l1.eval(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e2d15e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6, 6, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bebd2480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2, 2, 2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = Conv2D(ksize=5, filters=2, input_shape=o.shape, activation=Sigmoid(), stride=1, padding=0)\n",
    "l2.eval(o).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caed39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
