{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 36 \n",
    "Aug 20, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "\n",
    "    def __call__(self, ytrue, ypred):\n",
    "        return -np.sum( ytrue*np.log(ypred + 1e-10) + (1-ytrue)*np.log(1-ypred + 1e-10) )\n",
    "\n",
    "    def grad_input(self, y, ypred):\n",
    "        grad = np.zeros(ypred.shape)\n",
    "        ix0 = (y==0).reshape(-1,)\n",
    "        ix1 = (y==1).reshape(-1,)\n",
    "        grad[ix0,:] = 1/(1-ypred[ix0,:])\n",
    "        grad[ix1,:] = -1/ypred[ix1,:]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.eval(X)\n",
    "\n",
    "    def eval(self, X):\n",
    "        return 1/((np.e**-X) + 1)\n",
    "\n",
    "    def grad_input(self, X):\n",
    "        return np.identity(X.shape[1]) * self.eval(X)*(1-self.eval(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "[[2. 0.]\n",
      " [0. 3.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sigmoid()\n",
    "I = np.identity(2)\n",
    "b = np.array([2, 3])\n",
    "print(b)\n",
    "print(I)\n",
    "print(I*b)\n",
    "s.grad_input(np.array([[2, 3]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dot:\n",
    "\n",
    "    def __init__(self, input_size, units):\n",
    "        self.W = np.random.randn(input_size, units)\n",
    "        self.b = np.random.randn(1, units)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.eval(X)\n",
    "\n",
    "    def eval(self, X):\n",
    "        return X.dot(self.W) + self.b\n",
    "\n",
    "    def grad_input(self, X):\n",
    "        return self.W.T\n",
    "\n",
    "    def grad_w(self, X):\n",
    "        I = np.identity(self.b.shape[1])\n",
    "        g = np.stack([I]*self.W.shape[0], axis=0)\n",
    "        for i in range(g.shape[0]):\n",
    "            g[i] *= X[0][i]\n",
    "        return g\n",
    "\n",
    "    def grad_b(self):\n",
    "        return np.identity(self.b.shape[1])\n",
    "    \n",
    "    def get_parameter_shape(self):\n",
    "        return self.W.shape, self.b.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "\n",
    "    def __init__(self, input_size, activation, units):\n",
    "        \"\"\"\n",
    "        input_size: no. of neurons in previous layer\n",
    "        activation: some activation funtion\n",
    "        units: no. of neurons in current layer \n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "        self.units = units\n",
    "        self.dot = Dot(input_size, units)\n",
    "\n",
    "    def eval(self, X):\n",
    "        return self.activation( self.dot(X))\n",
    "\n",
    "    def grad_input(self, X):\n",
    "        g1 = self.activation.grad_input( self.dot(X) )\n",
    "        g2 = self.dot.grad_input(X)\n",
    "#         print(g1.shape, g2.shape, g1.dot(g2).shape)\n",
    "        return g1.dot(g2)\n",
    "\n",
    "    def grad_parameters(self, X):\n",
    "        da_dI = self.activation.grad_input(self.dot(X))\n",
    "        dI_dw = self.dot.grad_w(X)\n",
    "        da_dw = da_dI.dot(dI_dw)\n",
    "        \n",
    "        dI_db = self.dot.grad_b()\n",
    "        da_db = da_dI.dot(dI_db)\n",
    "        return np.transpose(da_dw, [1,0,2]), da_db\n",
    "        \n",
    "\n",
    "    def update(self, grad, optimizer):\n",
    "        \"\"\" grad: (dL_dwi, dL_dbi)\"\"\"\n",
    "        self.dot.W = optimizer.minimize(self.dot.W, grad[0])\n",
    "        self.dot.b = optimizer.minimize(self.dot.b, grad[1])\n",
    "        \n",
    "    def get_parameter_shape(self):\n",
    "        return self.dot.get_parameter_shape()\n",
    "    \n",
    "    def get_total_parameters(self):\n",
    "        w_shape, b_shape = self.dot.get_parameter_shape()\n",
    "        return np.prod(w_shape) + np.prod(b_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentOptimizer:\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def minimize(self, w, grad):\n",
    "        assert w.shape == grad.shape, f\"Shape mismatch w shape {w.shape} != grad shape {grad.shape}\"\n",
    "        w = w-self.lr*grad\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "\n",
    "    def __init__(self, loss):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        output = X\n",
    "        outputs = []\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            g = {}\n",
    "            g['input'] = layer.grad_input(output)\n",
    "            g['w'], g['b'] = layer.grad_parameters(output)\n",
    "            grads.append(g)\n",
    "            output = layer.eval(output)\n",
    "            outputs.append(output)\n",
    "        return outputs, grads\n",
    "\n",
    "    def back_propagate(self, grads, outputs, y):\n",
    "        grad_loss = self.loss.grad_input(y, outputs[-1]) # dL/dlast_layer_output\n",
    "        for layer, grad in list(zip(self.layers, grads))[::-1]:\n",
    "#             print(grad['input'].shape, grad['w'].shape, grad['b'].shape)\n",
    "            dL_dwi, dL_dbi = grad_loss.dot(grad['w']), grad_loss.dot(grad['b'])\n",
    "            layer.update((dL_dwi[0], dL_dbi), self.optimizer)\n",
    "            grad_loss = grad_loss.dot(grad['input']) # update grad loss for prev layer\n",
    "            \n",
    "\n",
    "    def fit(self, X, y, epochs, optimizer, learning_rate, verbose=1):\n",
    "        self.optimizer = optimizer(learning_rate)\n",
    "        for i in range(epochs):\n",
    "            outputs, grads = self.forward_propagation(X)\n",
    "            self.back_propagate(grads, outputs, y)\n",
    "            if verbose==1:\n",
    "                print(f\"\\rEpoch: {i+1} Loss: {self.loss(y, outputs[-1])}\", end=\"\")\n",
    "        if verbose==0:\n",
    "            print(f\"Epoch: {i} Loss: {self.loss(y, outputs[-1])}\")\n",
    "\n",
    "    def eval(self, X):\n",
    "        return self.forward_propagation(X)[0][-1]\n",
    "    \n",
    "    def summary(self):\n",
    "        from tabulate import tabulate\n",
    "        headers = [\"#\", \"Layer Type\", \"W.shape\", \"b.shape\", \"Total parameters\"]\n",
    "        table = []\n",
    "        total_p = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            w_shape, b_shape = layer.get_parameter_shape()\n",
    "            p = layer.get_total_parameters() # total parameters of a layer\n",
    "            table.append([i+1, layer.__class__.__name__, w_shape, b_shape, p])\n",
    "            total_p += p\n",
    "        print(tabulate(table, headers, tablefmt=\"pretty\"))\n",
    "        print(\"Total no. of model parameters\", total_p)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(BinaryCrossEntropy())\n",
    "model.add(Dense(input_size = 2, activation=Sigmoid(), units=3))\n",
    "model.add(Dense(input_size = 3, activation=Sigmoid(), units=2))\n",
    "model.add(Dense(input_size = 2, activation=Sigmoid(), units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+---------+------------------+\n",
      "| # | Layer Type | W.shape | b.shape | Total parameters |\n",
      "+---+------------+---------+---------+------------------+\n",
      "| 1 |   Dense    | (2, 3)  | (1, 3)  |        9         |\n",
      "| 2 |   Dense    | (3, 2)  | (1, 2)  |        8         |\n",
      "| 3 |   Dense    | (2, 1)  | (1, 1)  |        3         |\n",
      "+---+------------+---------+---------+------------------+\n",
      "Total no. of model parameters 20\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1, 2) Y: (1, 1)\n",
      "Loss: 0.24175454244267242\n",
      "Ypred: (1, 1) [[0.2147511]]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1\n",
    "n_features = 2\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = np.random.choice(a=[0,1], size=(1,n_samples))\n",
    "print(\"X:\", X.shape, \"Y:\", y.shape)\n",
    "ypred = model.eval(X)\n",
    "\n",
    "# before fitting/ training\n",
    "print(\"Loss:\", model.loss(y, ypred))\n",
    "print(\"Ypred:\", ypred.shape, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1500 Loss: 0.048172993179788345"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=1500, optimizer=GradientDescentOptimizer, learning_rate=0.008, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04700436]]\n"
     ]
    }
   ],
   "source": [
    "ypred = model.eval(X) # after training\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04814494977918185"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss(y, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d62006d1f3422635846181a997a61e8ec3049f797e5d7dfe0cd1bb84092b7c19"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
