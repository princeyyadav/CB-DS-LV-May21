{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a4490b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cc178451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \n",
    "    def __init__(self, ksize, filters, input_size, activation, stride=1, padding=0):\n",
    "        if input_size[0] <= 0 or input_size[1] <= 0:\n",
    "            raise ValueError(f\"Input image size is invalid, got {input_size}\")\n",
    "        self.ksize = ksize\n",
    "        self.filters = filters # no. of kernels in a layer -> no. of channels in each output\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.input_size = input_size # to decide no. of channels in the kernel\n",
    "        self.channels = input_size[-1]\n",
    "        self.activation = activation\n",
    "        self.kernels = []\n",
    "        for i in range(self.filters):\n",
    "            k = np.random.randn(ksize, ksize, self.channels)\n",
    "            self.kernels.append(k)\n",
    "        self.bias = np.random.randn(1,self.filters)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _rotate(inp):\n",
    "        assert len(inp.shape)==4, f\"No. of dim in inp not equal to 4, got {inp.shape}\"\n",
    "        return np.flip(inp, axis=(1,2))\n",
    "\n",
    "    @staticmethod\n",
    "    def _convolution_op_helper(inp, kernel, stride=1):\n",
    "        # inp shape -> 4 dim\n",
    "        assert len(inp.shape)==4, f\"No. of dim in inp not equal to 4, got {inp.shape}\"\n",
    "        # kernel shouldhave 4 dim\n",
    "        assert len(kernel.shape)==4, f\"No. of dim in kernel not equal to 4, got {kernel.shape}\"\n",
    "\n",
    "        # no. of chanels in kernel and that in inp it should be same\n",
    "        assert inp.shape[-1] == kernel.shape[-1], f\"Mismatch in no. of channels in inp and kernel, got inp {inp.shape[-1]}, kernel {kernel.shape[-1]}\"\n",
    "        # non-square kernels are not allowed\n",
    "        assert kernel.shape[1] == kernel.shape[2], f\"dim 0 of kernel doesn't match dim 1, got {kernel.shape}\"\n",
    "        # inp shape square\n",
    "        assert inp.shape[1]>=kernel.shape[1] and inp.shape[2]>=kernel.shape[2], f\"Inp map dim(1,2) < kernel dim(1,2), got inp map dim 1, 2 {inp.shape[1:-1]}, kernel dim 1,2 {kernel.shape[1:-1]}\"\n",
    "\n",
    "        # flip the kernel\n",
    "        kernel = Conv2D._rotate(kernel)\n",
    "\n",
    "        oup = []\n",
    "        start_rloc = 0\n",
    "        end_rloc = kernel.shape[1]\n",
    "        while end_rloc <= inp.shape[1]:\n",
    "            output = []\n",
    "            start_cloc = 0\n",
    "            end_cloc = kernel.shape[2]\n",
    "            while end_cloc <= inp.shape[2]:\n",
    "                conv = (inp[:,start_rloc:end_rloc, start_cloc:end_cloc]*kernel).sum(axis=(1,2,3))\n",
    "                output.append(conv)\n",
    "\n",
    "                start_cloc += stride\n",
    "                end_cloc += stride\n",
    "            oup.append(output)\n",
    "            start_rloc += stride\n",
    "            end_rloc += stride\n",
    "        return np.moveaxis(oup, -1, 0)\n",
    "    \n",
    "    def _convolution_op(self, inp):\n",
    "        output = []\n",
    "        for kernel in self.kernels:\n",
    "            o = Conv2D._convolution_op_helper(inp, np.expand_dims(kernel, axis=0), self.stride)\n",
    "            output.append(o)\n",
    "        output = np.stack(output, axis=-1)\n",
    "        return output\n",
    "    \n",
    "    def _pad_grad_I(self, grad_I):\n",
    "        return np.pad(grad_I, [(0, 0), (0, self.input_size[0] - grad_I.shape[1]), (0, self.input_size[1] - grad_I.shape[2]), (0,0)])\n",
    "            \n",
    "    @staticmethod\n",
    "    def _pad(inp, pad_width):   \n",
    "        assert len(inp.shape)==4, f\"No. of dim in inp not equal to 4, got {inp.shape}\"\n",
    "        return np.pad(inp, ((0,0), (pad_width,pad_width), (pad_width,pad_width), (0,0)))\n",
    "\n",
    "    @staticmethod\n",
    "    def _inside_pad(inp, pad_width):\n",
    "        assert len(inp.shape)==4, f\"No. of dim in inp not equal to 4, got {inp.shape}\"\n",
    "        ix = np.repeat(np.arange(1, inp.shape[1]), pad_width)\n",
    "        inp = np.insert(inp, ix, 0, axis=1)\n",
    "        return np.insert(inp, ix, 0, axis=2)\n",
    "        \n",
    "\n",
    "    def eval(self, X):\n",
    "        o_ = self._convolution_op(X) + self.bias\n",
    "        return self.activation(o_)\n",
    "\n",
    "    def grad_activation(self, X): #pqrs\n",
    "        o_ = self._convolution_op(X) + self.bias # shape: m, h, w, c; eg (50, 3,3,2)\n",
    "        m, h, w, c = o_.shape # (50, 2,2, 5)\n",
    "        do_do_ = self.activation.grad_input(o_.reshape(m, h*c*w)) # shape of do_do-: (50, 20, 20)\n",
    "        return np.diagonal(do_do_, axis1=1, axis2=2).reshape(o_.shape)\n",
    "    \n",
    "    \n",
    "    def gradient_dict(self, X):\n",
    "        g = {}\n",
    "        g['activation'] = self.grad_activation(X) # do_do_\n",
    "        g['input'] = self.get_input(X)\n",
    "        return g\n",
    "        \n",
    "    def get_input(self, X):\n",
    "        out_h, out_w, _ = self.get_output_size()\n",
    "        h = (out_h-1)*self.stride - 2*self.padding + self.ksize\n",
    "        w = (out_w-1)*self.stride - 2*self.padding + self.ksize\n",
    "        return Conv2D._rotate(X[:, :h, :w, :]) # flip input\n",
    "\n",
    "    def backprop_grad(self, grad_loss, grad): # abcd\n",
    "        # to find dL_dwi and dL_dbi, we need dL_do and do_do_. \n",
    "        \n",
    "        \"\"\"grad: dictionary, keys: activation, input\"\"\"\n",
    "        do_do_ = grad['activation'] # pqrs\n",
    "#         print(\"pqrs:\", do_do_.shape)\n",
    "        ##################################\n",
    "        #                                #\n",
    "        #          dL_dbi                #\n",
    "        #                                #\n",
    "        ##################################\n",
    "        b, h, w, c = grad_loss.shape\n",
    "        dL_do_ = grad_loss * do_do_[:,:h, :w,:]\n",
    "#         print(\"dL_do_\", dL_do_.shape)\n",
    "        dL_dbi = []\n",
    "        for c in range(dL_do_.shape[-1]):\n",
    "            b = dL_do_[:,:,:,c].sum(axis =(1, 2, 0))\n",
    "            dL_dbi.append(b)\n",
    "        dL_dbi = np.array(dL_dbi).reshape(1,-1)\n",
    "        \n",
    "        ##################################\n",
    "        #                                #\n",
    "        #          dL_dwi                #\n",
    "        #                                #\n",
    "        ##################################\n",
    "        kernels = Conv2D._inside_pad(dL_do_, self.stride-1) # abcd*pqrs -> act as a kernel while computing dL_dwi # 18,18,5\n",
    "        inps = grad['input'] # 20, 20,10\n",
    "#         print(\"grad_input:\", inps.shape)\n",
    "        dL_dwi = [] # len should be same no. of filters in this layer\n",
    "        for i in range(dL_do_.shape[-1]): # 5 times\n",
    "            kernel = kernels[:,:,:,i] # 1, 18,18, 1\n",
    "            dwi = []\n",
    "            for j in range(inps.shape[-1]): # 10 times\n",
    "                inp = inps[...,j] # 1, 20, 20, 1\n",
    "                conv = Conv2D._convolution_op_helper(np.expand_dims(inp,axis=-1) , np.expand_dims(kernel, axis=-1))\n",
    "                dwi.append(conv)\n",
    "#             print(conv.shape) # 10,m, 3 ,3\n",
    "            dwi = np.transpose(np.array(dwi), (1,0,2,3)).sum(axis=0) # (m, 10, 3, 3).sum(axis=0) -> 10, 3, 3\n",
    "            dwi = np.transpose(dwi, (1,2,0)) \n",
    "            dL_dwi.append(dwi)\n",
    "            \n",
    "        ##################################\n",
    "        #                                #\n",
    "        #          dL_dI                 #\n",
    "        #                                #\n",
    "        ##################################\n",
    "        inps = Conv2D._pad(kernels, self.ksize-1)\n",
    "        kernels = self.kernels\n",
    "        dL_dI = []\n",
    "        for i in range(self.input_size[-1]):\n",
    "            ## ith channel of jth kernel needs to convolve with jth channel of inp \n",
    "            kernel = [self.kernels[j][...,i] for j in range(len(self.kernels))]\n",
    "            kernel = np.stack(kernel, axis=-1) # 3,3,5\n",
    "            conv = Conv2D._convolution_op_helper(inps, np.expand_dims(kernel, axis=0))\n",
    "            dL_dI.append(conv)\n",
    "        dL_dI = np.stack(dL_dI, axis=-1)\n",
    "        \n",
    "        return dL_dwi, dL_dbi, self._pad_grad_I(dL_dI)\n",
    "    \n",
    "    def _pad_grad_I(self, grad_I):\n",
    "        return np.pad(grad_I, [(0, 0), (0, self.input_size[0] - grad_I.shape[1]), (0, self.input_size[1] - grad_I.shape[2]), (0,0)])\n",
    "        \n",
    "    def update(self, grad, optimizer):\n",
    "        \"\"\" grad: (dL_dwi, dL_dbi)\"\"\"\n",
    "        self.bias = optimizer.minimize(self.bias, grad[1])\n",
    "        for i in range(len(self.kernels)):\n",
    "            self.kernels[i] = optimizer.minimize(self.kernels[i], grad[0][i]) \n",
    "            \n",
    "    def get_parameter_shape(self):\n",
    "        return self.kernels[0].shape, self.bias.shape\n",
    "    \n",
    "    def get_output_size(self):\n",
    "        m, n, k, p, s = self.input_size[0], self.input_size[1], self.ksize, self.padding, self.stride\n",
    "        return ((m-k+(2*p))//s)+1, ((n-k+(2*p))//s)+1, self.filters\n",
    "    \n",
    "    def get_total_parameters(self):\n",
    "        return np.prod((len(self.kernels), *self.kernels[0].shape)) + np.prod(self.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc1a8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.eval(X)\n",
    "\n",
    "    def eval(self, X):\n",
    "        return 1/((np.e**-X) + 1)\n",
    "\n",
    "    def grad_input(self, X):\n",
    "        I = np.identity(X.shape[1])\n",
    "        b = self.eval(X)*(1-self.eval(X)) # same shape as X\n",
    "        return np.einsum('ij,mi->mij', I, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aaf1212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        if input_size[0] <= 0 or input_size[1] <= 0:\n",
    "            raise ValueError(f\"Input image size is invalid, got {input_size}\")\n",
    "        self.h, self.w, self.c = input_size\n",
    "        \n",
    "    def eval(self, X):\n",
    "        return X.reshape(-1, self.h*self.w*self.c)\n",
    "    \n",
    "    \n",
    "    def gradient_dict(self, X):\n",
    "        return {}\n",
    "\n",
    "    def backprop_grad(self, grad_loss, grad): # abcd (grad_loss: 10,1,1620)\n",
    "        return None, None, grad_loss[:, 0, :].reshape(-1, self.h, self.w, self.c) # -> 10, 18, 18, 5 \n",
    "        \n",
    "    def update(self, grad, optimizer):\n",
    "        \"\"\" grad: (dL_dwi, dL_dbi)\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def get_parameter_shape(self):\n",
    "        return (\"-\",\"-\")\n",
    "    \n",
    "    def get_output_size(self):\n",
    "        return (1,self.h*self.w*self.c)\n",
    "    \n",
    "    def get_total_parameters(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7aa40d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 18, 18, 5)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 5\n",
    "f = 2\n",
    "np.ones((10, 1, 1620))[:,0,:].reshape((-1, 18, 18, 5)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13dfc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"E://CB-DS-LV-May21//DS//NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af299340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation import Sigmoid\n",
    "from loss import BinaryCrossEntropy\n",
    "from optimizer import GradientDescentOptimizer\n",
    "from layer import Dense\n",
    "from model import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc7b88d",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cd2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7a880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a35de9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12665, 28, 28, 1) (12665, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train[(y_train==0) | (y_train==1)].reshape(-1, 28,28,1)/255\n",
    "y_train = y_train[(y_train==0) | (y_train==1)].reshape(-1,1)\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cda329b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+---------+--------------+------------------+\n",
      "| # | Layer Type |  W.shape   | b.shape | Output shape | Total parameters |\n",
      "+---+------------+------------+---------+--------------+------------------+\n",
      "| 1 |   Conv2D   | (5, 5, 1)  | (1, 20) | (12, 12, 20) |       520        |\n",
      "| 2 |   Conv2D   | (5, 5, 20) | (1, 10) |  (4, 4, 10)  |       5010       |\n",
      "| 3 |   Conv2D   | (3, 3, 10) | (1, 5)  |  (2, 2, 5)   |       455        |\n",
      "| 4 |   Conv2D   | (2, 2, 5)  | (1, 5)  |  (1, 1, 5)   |       105        |\n",
      "| 5 |  Flatten   |     -      |    -    |    (1, 5)    |        0         |\n",
      "| 6 |   Dense    |   (5, 1)   | (1, 1)  |    (1, 1)    |        6         |\n",
      "+---+------------+------------+---------+--------------+------------------+\n",
      "Total no. of model parameters 6096\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(BinaryCrossEntropy())\n",
    "model.add(Conv2D, ksize=5, filters=20, input_size=(28,28,1), activation=Sigmoid(), stride=2, padding=0)\n",
    "model.add(Conv2D, ksize=5, filters=10, activation=Sigmoid(), stride=2, padding=0)\n",
    "model.add(Conv2D, ksize=3, filters=5, activation=Sigmoid(), stride=1, padding=0)\n",
    "model.add(Conv2D, ksize=2, filters=5, activation=Sigmoid(), stride=1, padding=0)\n",
    "model.add(Flatten)\n",
    "model.add(Dense, activation=Sigmoid(), units=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cca8a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 28, 28, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e1cfd5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(x_train[0:10])\n",
    "print(ypred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "266b9544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130.9201092810309"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss(ypred, y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "00988fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 1.20282292144554334"
     ]
    }
   ],
   "source": [
    "model.fit(x_train[:10], y_train[:10], epochs=3, optimizer=GradientDescentOptimizer, learning_rate=0.005, verbose=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "63118744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(x_train[0:10])\n",
    "print(ypred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "380cfdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.64618569567816"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss(ypred, y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635d4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
