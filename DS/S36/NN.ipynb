{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 36 \n",
    "Aug 20, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "\n",
    "    def __call__(self, ytrue, ypred):\n",
    "        return -np.sum( ytrue*np.log(ypred + 1e-10) + (1-ytrue)*np.log(1-ypred + 1e-10) )\n",
    "\n",
    "    def grad_input(self, y, ypred):\n",
    "        if y == 0:\n",
    "            return 1/1-ypred\n",
    "        else:\n",
    "            return -1/ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.eval(X)\n",
    "\n",
    "    def eval(self, X):\n",
    "        return 1/((np.e**-X) + 1)\n",
    "\n",
    "    def grad_input(self, X):\n",
    "        return np.identity(X.shape[1])*self.eval(X) * (1-self.eval(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dot:\n",
    "\n",
    "    def __init__(self, input_size, units):\n",
    "        self.W = np.random.randn(input_size, units)\n",
    "        self.b = np.random.randn(1, units)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.eval(X)\n",
    "\n",
    "    def eval(self, X):\n",
    "        return X.dot(self.W) + self.b\n",
    "\n",
    "    def grad_input(self):\n",
    "        return self.W.T\n",
    "\n",
    "    def grad_w(self, X):\n",
    "        I = np.identity(self.b.shape[1])\n",
    "        g = np.stack([I]*self.W.shape[0])\n",
    "        for i in range(g.shape[0]):\n",
    "            g[i] *= X[0][i]\n",
    "        return g\n",
    "\n",
    "    def grad_b(self):\n",
    "        return np.identity(self.b.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "\n",
    "    def __init__(self, input_size, activation, units):\n",
    "        \"\"\"\n",
    "        input_size: no. of neurons in previous layer\n",
    "        activation: some activation funtion\n",
    "        units: no. of neurons in current layer \n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "        self.units = units\n",
    "        self.dot = Dot(input_size, units)\n",
    "\n",
    "    def eval(self, X):\n",
    "        return self.activation( self.dot(X))\n",
    "\n",
    "    def grad_input(self, X):\n",
    "        g1 = self.activation.grad_input( self.dot(X) )\n",
    "        g2 = self.dot.grad_input()\n",
    "#         print(\"g1\",g1.shape, \"g2\", g2.shape)\n",
    "        return g1.dot(g2)\n",
    "\n",
    "    def grad_parameters(self, X):\n",
    "        da_dI = self.activation.grad_input(self.dot(X))\n",
    "        dI_dw = self.dot.grad_w(X)\n",
    "        # da_dw = da_dI.dot(dI_dw)\n",
    "#         print(\"da_dI\\n:\", da_dI)\n",
    "#         print(\"dI_dw\\n:\", da_dI)\n",
    "#         print(f\"da_dI: {da_dI.shape}, dI_dw {dI_dw.shape}\")\n",
    "\n",
    "        da_dw = da_dI * dI_dw\n",
    "#         print(\"da_dI\\n:\", da_dI)\n",
    "#         print(f\"da_dI: {da_dI.shape}, dI_dw {dI_dw.shape}, da_dw{da_dw.shape}\")\n",
    "\n",
    "        dI_db = self.dot.grad_b()\n",
    "        da_db = da_dI * dI_db\n",
    "#         print(f\"da_dI: {da_dI.shape}, dI_db {dI_db.shape}, da_db{da_db.shape}\")\n",
    "\n",
    "        return da_dw, da_db\n",
    "\n",
    "    def update(self, grad, optimizer):\n",
    "        \"\"\" grad: (dL_dwi, dL_dbi)\"\"\"\n",
    "        self.dot.W = optimizer.minimize(self.dot.W, grad[0])\n",
    "        self.dot.b = optimizer.minimize(self.dot.b, grad[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentOptimizer:\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def minimize(self, w, grad):\n",
    "        assert w.shape == grad.shape, f\"Shape mismatch w shape {w.shape} != grad shape {grad.shape}\"\n",
    "        w = w-self.lr*grad\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "\n",
    "    def __init__(self, loss):\n",
    "        self.layers = []\n",
    "        self.loss = loss\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        output = X\n",
    "        outputs = []\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            g = {}\n",
    "            g['input'] = layer.grad_input(output)\n",
    "            g['w'], g['b'] = layer.grad_parameters(output)\n",
    "            grads.append(g)\n",
    "            output = layer.eval(output)\n",
    "            outputs.append(output)\n",
    "#             print(\"ForProp:\", output.shape)\n",
    "        return outputs, grads\n",
    "\n",
    "    def back_propagate(self, grads, outputs, y):\n",
    "        grad_loss = self.loss.grad_input(y, outputs[-1]) # dL/dlast_layer_output\n",
    "        for layer, grad in list(zip(self.layers, grads))[::-1]:\n",
    "            dL_dwi, dL_dbi = grad_loss.dot(grad['w']), grad_loss.dot(grad['b'])\n",
    "            layer.update((dL_dwi[0], dL_dbi), self.optimizer)\n",
    "            grad_loss = grad_loss.dot(grad['input']) # update grad loss for prev layer\n",
    "            \n",
    "\n",
    "    def fit(self, X, y, epochs, optimizer, learning_rate, verbose=1):\n",
    "        self.optimizer = optimizer(learning_rate)\n",
    "        for i in range(epochs):\n",
    "            outputs, grads = self.forward_propagation(X)\n",
    "            self.back_propagate(grads, outputs, y)\n",
    "            if verbose==1:\n",
    "                print(f\"\\rEpoch: {i+1} Loss: {self.loss(y, outputs[-1])}\", end=\"\")\n",
    "        if verbose==0:\n",
    "            print(f\"Epoch: {i} Loss: {self.loss(y, outputs[-1])}\")\n",
    "\n",
    "    def eval(self, X):\n",
    "        return self.forward_propagation(X)[0][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(BinaryCrossEntropy())\n",
    "model.add(Dense(input_size = 2, activation=Sigmoid(), units=2))\n",
    "model.add(Dense(input_size = 2, activation=Sigmoid(), units=3))\n",
    "model.add(Dense(input_size = 3, activation=Sigmoid(), units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39710514 -1.06285403]]\n",
      "[[0.26351748]]\n",
      "0.30586977626079315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.26351748]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.randn(1,2)\n",
    "y = np.array([0])\n",
    "print(X)\n",
    "ypred = model.eval(X)\n",
    "print(ypred)\n",
    "\n",
    "# before fitting/ training\n",
    "print(model.loss(y, ypred))\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 Loss: 0.01849475873446689"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=1000, optimizer=GradientDescentOptimizer, learning_rate=0.008, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01831834]]\n"
     ]
    }
   ],
   "source": [
    "ypred = model.eval(X) # after training\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d62006d1f3422635846181a997a61e8ec3049f797e5d7dfe0cd1bb84092b7c19"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
